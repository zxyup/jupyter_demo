{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    " \n",
    "wine.data\n",
    "wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实例化\n",
    "#训练集带入实例化的模型去进行训练，使用的接口是fit\n",
    "#使用其他接口将测试集导入我们训练好的模型，去获取我们希望过去的结果（score.Y_test）\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data,wine.target,test_size=0.3)\n",
    " \n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "clf = clf.fit(Xtrain,Ytrain)\n",
    "rfc = rfc.fit(Xtrain,Ytrain)\n",
    "score_c = clf.score(Xtest,Ytest)\n",
    "score_r = rfc.score(Xtest,Ytest)\n",
    " \n",
    "print(\"Single Tree:{}\".format(score_c)\n",
    "      ,\"Random Forest:{}\".format(score_r)\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#目的是带大家复习一下交叉验证\n",
    "#交叉验证：是数据集划分为n分，依次取每一份做测试集，每n-1份做训练集，多次训练模型以观测模型稳定性的方法\n",
    " \n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "rfc = RandomForestClassifier(n_estimators=25)\n",
    "rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=10)\n",
    " \n",
    "clf = DecisionTreeClassifier()\n",
    "clf_s = cross_val_score(clf,wine.data,wine.target,cv=10)\n",
    " \n",
    "plt.plot(range(1,11),rfc_s,label = \"RandomForest\")\n",
    "plt.plot(range(1,11),clf_s,label = \"Decision Tree\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    " \n",
    "#====================一种更加有趣也更简单的写法===================#\n",
    " \n",
    "\"\"\"\n",
    "label = \"RandomForest\"\n",
    "for model in [RandomForestClassifier(n_estimators=25),DecisionTreeClassifier()]:\n",
    "    score = cross_val_score(model,wine.data,wine.target,cv=10)\n",
    "    print(\"{}:\".format(label)),print(score.mean())\n",
    "    plt.plot(range(1,11),score,label = label)\n",
    "    plt.legend()\n",
    "    label = \"DecisionTree\"\n",
    " \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_l = []\n",
    "clf_l = []\n",
    " \n",
    "for i in range(10):\n",
    "    rfc = RandomForestClassifier(n_estimators=25)\n",
    "    rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=10).mean()\n",
    "    rfc_l.append(rfc_s)\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf_s = cross_val_score(clf,wine.data,wine.target,cv=10).mean()\n",
    "    clf_l.append(clf_s)\n",
    "    \n",
    "plt.plot(range(1,11),rfc_l,label = \"Random Forest\")\n",
    "plt.plot(range(1,11),clf_l,label = \"Decision Tree\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    " \n",
    "#是否有注意到，单个决策树的波动轨迹和随机森林一致？\n",
    "#再次验证了我们之前提到的，单个决策树的准确率越高，随机森林的准确率也会越高\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####【TIME WARNING: 2mins 30 seconds】#####\n",
    " \n",
    "superpa = []\n",
    "for i in range(200):\n",
    "    rfc = RandomForestClassifier(n_estimators=i+1,n_jobs=-1)\n",
    "    rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=10).mean()\n",
    "    superpa.append(rfc_s)\n",
    "print(max(superpa),superpa.index(max(superpa))+1)#打印出：最高精确度取值，max(superpa))+1指的是森林数目的数量n_estimators\n",
    "plt.figure(figsize=[20,5])\n",
    "plt.plot(range(1,201),superpa)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import comb\n",
    " \n",
    "np.array([comb(25,i)*(0.2**i)*((1-0.2)**(25-i)) for i in range(13,26)]).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=20,random_state=2)\n",
    "rfc = rfc.fit(Xtrain, Ytrain)\n",
    " \n",
    "#随机森林的重要属性之一：estimators，查看森林中树的状况\n",
    "rfc.estimators_[0].random_state\n",
    " \n",
    "for i in range(len(rfc.estimators_)):\n",
    "    print(rfc.estimators_[i].random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#无需划分训练集和测试集\n",
    " \n",
    "rfc = RandomForestClassifier(n_estimators=25,oob_score=True)#默认为False\n",
    "rfc = rfc.fit(wine.data,wine.target)\n",
    " \n",
    "#重要属性oob_score_\n",
    "rfc.oob_score_#0.9719101123595506\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#大家可以分别取尝试一下这些属性和接口\n",
    " \n",
    "rfc = RandomForestClassifier(n_estimators=25)\n",
    "rfc = rfc.fit(Xtrain, Ytrain)\n",
    "rfc.score(Xtest,Ytest)\n",
    " \n",
    "rfc.feature_importances_#结合zip可以对照特征名字查看特征重要性，参见上节决策树\n",
    "rfc.apply(Xtest)#apply返回每个测试样本所在的叶子节点的索引\n",
    "rfc.predict(Xtest)#predict返回每个测试样本的分类/回归结果\n",
    "rfc.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "x = np.linspace(0,1,20)\n",
    " \n",
    "y = []\n",
    "for epsilon in np.linspace(0,1,20):\n",
    "    E = np.array([comb(25,i)*(epsilon**i)*((1-epsilon)**(25-i)) for i in range(13,26)]).sum()      \n",
    "    y.append(E)\n",
    "plt.plot(x,y,\"o-\",label=\"when estimators are different\")\n",
    "plt.plot(x,x,\"--\",color=\"red\",label=\"if all estimators are same\")\n",
    "plt.xlabel(\"individual estimator's error\")\n",
    "plt.ylabel(\"RandomForest's error\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "x = np.linspace(0,1,20)\n",
    " \n",
    "y = []\n",
    "for epsilon in np.linspace(0,1,20):\n",
    "    E = np.array([comb(25,i)*(epsilon**i)*((1-epsilon)**(25-i)) \n",
    "                  for i in range(13,26)]).sum()\n",
    "    y.append(E)\n",
    "plt.plot(x,y,\"o-\",label=\"when estimators are different\")\n",
    "plt.plot(x,x,\"--\",color=\"red\",label=\"if all estimators are same\")\n",
    "plt.xlabel(\"individual estimator's error\")\n",
    "plt.ylabel(\"RandomForest's error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston#一个标签是连续西变量的数据集\n",
    "from sklearn.model_selection import cross_val_score#导入交叉验证模块\n",
    "from sklearn.ensemble import RandomForestRegressor#导入随机森林回归系\n",
    " \n",
    "boston = load_boston()\n",
    "regressor = RandomForestRegressor(n_estimators=100,random_state=0)#实例化\n",
    "cross_val_score(regressor, boston.data, boston.target, cv=10\n",
    "               ,scoring = \"neg_mean_squared_error\"#如果不写scoring，回归评估默认是R平方\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn当中的模型评估指标（打分）列表\n",
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())#这些指标是scoring可选择的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.impute import SimpleImputer#填补缺失值的类\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_boston()\n",
    "dataset#是一个字典\n",
    "dataset.target#查看数据标签\n",
    "dataset.data#数据的特征矩阵\n",
    "dataset.data.shape#数据的结构\n",
    "#总共506*13=6578个数据\n",
    " \n",
    "X_full, y_full = dataset.data, dataset.target\n",
    "n_samples = X_full.shape[0]#506\n",
    "n_features = X_full.shape[1]#13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#首先确定我们希望放入的缺失数据的比例，在这里我们假设是50%，那总共就要有3289个数据缺失\n",
    " \n",
    "rng = np.random.RandomState(0)#设置一个随机种子，方便观察\n",
    "missing_rate = 0.5\n",
    "n_missing_samples = int(np.floor(n_samples * n_features * missing_rate))#3289\n",
    "#np.floor向下取整，返回.0格式的浮点数\n",
    " \n",
    "#所有数据要随机遍布在数据集的各行各列当中，而一个缺失的数据会需要一个行索引和一个列索引\n",
    "#如果能够创造一个数组，包含3289个分布在0~506中间的行索引，和3289个分布在0~13之间的列索引，那我们就可以利用索引来为数据中的任意3289个位置赋空值\n",
    "#然后我们用0，均值和随机森林来填写这些缺失值，然后查看回归的结果如何\n",
    " \n",
    "missing_features = rng.randint(0,n_features,n_missing_samples)#randint（下限，上限，n）指在下限和上限之间取出n个整数\n",
    "len(missing_features)#3289\n",
    "missing_samples = rng.randint(0,n_samples,n_missing_samples)\n",
    "len(missing_samples)#3289\n",
    " \n",
    "#missing_samples = rng.choice(n_samples,n_missing_samples,replace=False)\n",
    "#我们现在采样了3289个数据，远远超过我们的样本量506，所以我们使用随机抽取的函数randint。\n",
    "# 但如果我们需要的数据量小于我们的样本量506，那我们可以采用np.random.choice来抽样，choice会随机抽取不重复的随机数，\n",
    "# 因此可以帮助我们让数据更加分散，确保数据不会集中在一些行中!\n",
    "#这里我们不采用np.random.choice,因为我们现在采样了3289个数据，远远超过我们的样本量506，使用np.random.choice会报错\n",
    " \n",
    "X_missing = X_full.copy()\n",
    "y_missing = y_full.copy()\n",
    " \n",
    "X_missing[missing_samples,missing_features] = np.nan\n",
    " \n",
    "X_missing = pd.DataFrame(X_missing)\n",
    "#转换成DataFrame是为了后续方便各种操作，numpy对矩阵的运算速度快到拯救人生，但是在索引等功能上却不如pandas来得好用\n",
    "X_missing.head()\n",
    "\n",
    "#并没有对y_missing进行缺失值填补，原因是有监督学习，不能缺标签啊\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用均值进行填补\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')#实例化\n",
    "X_missing_mean = imp_mean.fit_transform(X_missing)#特殊的接口fit_transform = 训练fit + 导出predict\n",
    "#pd.DataFrame(X_missing_mean).isnull()#但是数据量大的时候还是看不全\n",
    "#布尔值False = 0， True = 1 \n",
    "# pd.DataFrame(X_missing_mean).isnull().sum()#如果求和为0可以彻底确认是否有NaN\n",
    "\n",
    "#使用0进行填补\n",
    "imp_0 = SimpleImputer(missing_values=np.nan, strategy=\"constant\",fill_value=0)#constant指的是常数\n",
    "X_missing_0 = imp_0.fit_transform(X_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_missing_reg = X_missing.copy()\n",
    "\n",
    "#找出数据集中，缺失值从小到大排列的特征们的顺序，并且有了这些的索引\n",
    "sortindex = np.argsort(X_missing_reg.isnull().sum(axis=0)).values#np.argsort()返回的是从小到大排序的顺序所对应的索引\n",
    " \n",
    "for i in sortindex:\n",
    "    \n",
    "    #构建我们的新特征矩阵（没有被选中去填充的特征 + 原始的标签）和新标签（被选中去填充的特征）\n",
    "    df = X_missing_reg\n",
    "    fillc = df.iloc[:,i]#新标签\n",
    "    df = pd.concat([df.iloc[:,df.columns != i],pd.DataFrame(y_full)],axis=1)#新特征矩阵\n",
    "    \n",
    "    #在新特征矩阵中，对含有缺失值的列，进行0的填补\n",
    "    df_0 =SimpleImputer(missing_values=np.nan,strategy='constant',fill_value=0).fit_transform(df)\n",
    "                        \n",
    "    #找出我们的训练集和测试集\n",
    "    Ytrain = fillc[fillc.notnull()]# Ytrain是被选中要填充的特征中（现在是我们的标签），存在的那些值：非空值\n",
    "    Ytest = fillc[fillc.isnull()]#Ytest 是被选中要填充的特征中（现在是我们的标签），不存在的那些值：空值。注意我们需要的不是Ytest的值，需要的是Ytest所带的索引\n",
    "    Xtrain = df_0[Ytrain.index,:]#在新特征矩阵上，被选出来的要填充的特征的非空值所对应的记录\n",
    "    Xtest = df_0[Ytest.index,:]#在新特征矩阵上，被选出来的要填充的特征的空值所对应的记录\n",
    "    \n",
    "    #用随机森林回归来填补缺失值\n",
    "    rfc = RandomForestRegressor(n_estimators=100)#实例化\n",
    "    rfc = rfc.fit(Xtrain, Ytrain)#导入训练集进行训练\n",
    "    Ypredict = rfc.predict(Xtest)#用predict接口将Xtest导入，得到我们的预测结果（回归结果），就是我们要用来填补空值的这些值\n",
    "    \n",
    "    #将填补好的特征返回到我们的原始的特征矩阵中\n",
    "    X_missing_reg.loc[X_missing_reg.iloc[:,i].isnull(),i] = Ypredict\n",
    "\n",
    "#检验是否有空值\n",
    "X_missing_reg.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对所有数据进行建模，取得MSE结果\n",
    " \n",
    "X = [X_full,X_missing_mean,X_missing_0,X_missing_reg]\n",
    " \n",
    "mse = []\n",
    "std = []\n",
    "for x in X:\n",
    "    estimator = RandomForestRegressor(random_state=0, n_estimators=100)#实例化\n",
    "    scores = cross_val_score(estimator,x,y_full,scoring='neg_mean_squared_error', cv=5).mean()\n",
    "    mse.append(scores * -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*zip(['Full data','Zero Imputation','Mean Imputation','Regressor Imputation'],mse)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = ['Full data',\n",
    "            'Zero Imputation',\n",
    "            'Mean Imputation',\n",
    "            'Regressor Imputation']\n",
    "colors = ['r', 'g', 'b', 'orange']\n",
    " \n",
    "plt.figure(figsize=(12, 6))#画出画布\n",
    "ax = plt.subplot(111)#添加子图\n",
    "for i in np.arange(len(mse)):\n",
    "    ax.barh(i, mse[i],color=colors[i], alpha=0.6, align='center')#bar为条形图，barh为横向条形图，alpha表示条的粗度\n",
    "ax.set_title('Imputation Techniques with Boston Data')\n",
    "ax.set_xlim(left=np.min(mse) * 0.9,\n",
    "             right=np.max(mse) * 1.1)#设置x轴取值范围\n",
    "ax.set_yticks(np.arange(len(mse)))\n",
    "ax.set_xlabel('MSE')\n",
    "ax.set_yticklabels(x_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    " \n",
    "data\n",
    " \n",
    "data.data.shape\n",
    " \n",
    "data.target\n",
    " \n",
    "#可以看到，乳腺癌数据集有569条记录，30个特征，单看维度虽然不算太高，但是样本量非常少。过拟合的情况可能存在。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100,random_state=90)\n",
    "score_pre = cross_val_score(rfc,data.data,data.target,cv=10).mean()#交叉验证的分类默认scoring='accuracy'\n",
    " \n",
    "score_pre\n",
    " \n",
    "#这里可以看到，随机森林在乳腺癌数据上的表现本就还不错，在现实数据集上，基本上不可能什么都不调就看到95%以上的准确率\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "在这里我们选择学习曲线，可以使用网格搜索吗？可以，但是只有学习曲线，才能看见趋势\n",
    "我个人的倾向是，要看见n_estimators在什么取值开始变得平稳，是否一直推动模型整体准确率的上升等信息\n",
    "第一次的学习曲线，可以先用来帮助我们划定范围，我们取每十个数作为一个阶段，来观察n_estimators的变化如何\n",
    "引起模型整体准确率的变化\n",
    "\"\"\"\n",
    " \n",
    "#####【TIME WARNING: 30 seconds】#####\n",
    " \n",
    "scorel = []\n",
    "for i in range(0,200,10):\n",
    "    rfc = RandomForestClassifier(n_estimators=i+1,\n",
    "                                 n_jobs=-1,\n",
    "                                 random_state=90)\n",
    "    score = cross_val_score(rfc,data.data,data.target,cv=10).mean()\n",
    "    scorel.append(score)\n",
    "print(max(scorel),(scorel.index(max(scorel))*10)+1)\n",
    "plt.figure(figsize=[20,5])\n",
    "plt.plot(range(1,201,10),scorel)\n",
    "plt.show()\n",
    " \n",
    "#list.index([object])\n",
    "#返回这个object在列表list中的索引\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorel = []\n",
    "for i in range(35,45):\n",
    "    rfc = RandomForestClassifier(n_estimators=i,\n",
    "                                 n_jobs=-1,\n",
    "                                 random_state=90)\n",
    "    score = cross_val_score(rfc,data.data,data.target,cv=10).mean()\n",
    "    scorel.append(score)\n",
    "print(max(scorel),([*range(35,45)][scorel.index(max(scorel))]))\n",
    "plt.figure(figsize=[20,5])\n",
    "plt.plot(range(35,45),scorel)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "有一些参数是没有参照的，很难说清一个范围，这种情况下我们使用学习曲线，看趋势\n",
    "从曲线跑出的结果中选取一个更小的区间，再跑曲线\n",
    "param_grid = {'n_estimators':np.arange(0, 200, 10)}\n",
    " \n",
    "param_grid = {'max_depth':np.arange(1, 20, 1)}\n",
    "    \n",
    "param_grid = {'max_leaf_nodes':np.arange(25,50,1)}\n",
    "    对于大型数据集，可以尝试从1000来构建，先输入1000，每100个叶子一个区间，再逐渐缩小范围\n",
    " \n",
    "有一些参数是可以找到一个范围的，或者说我们知道他们的取值和随着他们的取值，模型的整体准确率会如何变化，这\n",
    "样的参数我们就可以直接跑网格搜索\n",
    "param_grid = {'criterion':['gini', 'entropy']}\n",
    " \n",
    "param_grid = {'min_samples_split':np.arange(2, 2+20, 1)}\n",
    " \n",
    "param_grid = {'min_samples_leaf':np.arange(1, 1+10, 1)}\n",
    "    \n",
    "param_grid = {'max_features':np.arange(5,30,1)} \n",
    " \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调整max_depth\n",
    " \n",
    "param_grid = {'max_depth':np.arange(1, 20, 1)}\n",
    " \n",
    "#   一般根据数据的大小来进行一个试探，乳腺癌数据很小，所以可以采用1~10，或者1~20这样的试探\n",
    "#   但对于像digit recognition那样的大型数据来说，我们应该尝试30~50层深度（或许还不足够\n",
    "#   更应该画出学习曲线，来观察深度对模型的影响\n",
    " \n",
    "rfc = RandomForestClassifier(n_estimators=39\n",
    "                             ,random_state=90\n",
    "                            )\n",
    "GS = GridSearchCV(rfc,param_grid,cv=10)#网格搜索\n",
    "GS.fit(data.data,data.target)\n",
    " \n",
    "GS.best_params_#显示调整出来的最佳参数\n",
    " \n",
    "GS.best_score_#返回调整好的最佳参数对应的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调整max_features\n",
    " \n",
    "param_grid = {'max_features':np.arange(5,30,1)} \n",
    " \n",
    "\"\"\"\n",
    " \n",
    "max_features是唯一一个即能够将模型往左（低方差高偏差）推，也能够将模型往右（高方差低偏差）推的参数。我\n",
    "们需要根据调参前，模型所在的位置（在泛化误差最低点的左边还是右边）来决定我们要将max_features往哪边调。\n",
    "现在模型位于图像左侧，我们需要的是更高的复杂度，因此我们应该把max_features往更大的方向调整，可用的特征\n",
    "越多，模型才会越复杂。max_features的默认最小值是sqrt(n_features)，因此我们使用这个值作为调参范围的\n",
    "最小值。\n",
    " \n",
    "\"\"\"\n",
    " \n",
    "rfc = RandomForestClassifier(n_estimators=39\n",
    "                             ,random_state=90\n",
    "                            )\n",
    "GS = GridSearchCV(rfc,param_grid,cv=10)\n",
    "GS.fit(data.data,data.target)\n",
    " \n",
    "GS.best_params_\n",
    " \n",
    "GS.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调整min_samples_leaf\n",
    " \n",
    "param_grid={'min_samples_leaf':np.arange(1, 1+10, 1)}\n",
    " \n",
    "#对于min_samples_split和min_samples_leaf,一般是从他们的最小值开始向上增加10或20\n",
    "#面对高维度高样本量数据，如果不放心，也可以直接+50，对于大型数据，可能需要200~300的范围\n",
    "#如果调整的时候发现准确率无论如何都上不来，那可以放心大胆调一个很大的数据，大力限制模型的复杂度\n",
    " \n",
    "rfc = RandomForestClassifier(n_estimators=39\n",
    "                             ,random_state=90\n",
    "                            )\n",
    "GS = GridSearchCV(rfc,param_grid,cv=10)\n",
    "GS.fit(data.data,data.target)\n",
    " \n",
    "GS.best_params_\n",
    " \n",
    "GS.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调整min_samples_split\n",
    " \n",
    "param_grid={'min_samples_split':np.arange(2, 2+20, 1)}\n",
    " \n",
    "rfc = RandomForestClassifier(n_estimators=39\n",
    "                             ,random_state=90\n",
    "                            )\n",
    "GS = GridSearchCV(rfc,param_grid,cv=10)\n",
    "GS.fit(data.data,data.target)\n",
    " \n",
    "GS.best_params_\n",
    " \n",
    "GS.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调整Criterion\n",
    " \n",
    "param_grid = {'criterion':['gini', 'entropy']}\n",
    " \n",
    "rfc = RandomForestClassifier(n_estimators=39\n",
    "                             ,random_state=90\n",
    "                            )\n",
    "GS = GridSearchCV(rfc,param_grid,cv=10)\n",
    "GS.fit(data.data,data.target)\n",
    " \n",
    "GS.best_params_\n",
    " \n",
    "GS.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=39,random_state=90)\n",
    "score = cross_val_score(rfc,data.data,data.target,cv=10).mean()\n",
    "score\n",
    " \n",
    "score - score_pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a241fd89caf15c2b619e4f917aceb329b85e81f609a85e69a2775cd56dbd70ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
